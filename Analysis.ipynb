{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c046b79-9027-4b68-a97c-c508620fa0ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Creating SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Uber Data Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faaa019f-5331-4aaa-aa67-8892580159c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading Uber Trip Dataset\n",
    "df_trip = spark.read.csv(\"dataset.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3c82c1-34a9-4980-bf4a-70cffcafb3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+---------+-------+----------------+---------+--------------+\n",
      "|     Date|Time (Local)|Eyeballs |Zeroes |Completed Trips |Requests |Unique Drivers|\n",
      "+---------+------------+---------+-------+----------------+---------+--------------+\n",
      "|10-Sep-12|           7|        5|      0|               2|        2|             9|\n",
      "|     NULL|           8|        6|      0|               2|        2|            14|\n",
      "|     NULL|           9|        8|      3|               0|        0|            14|\n",
      "|     NULL|          10|        9|      2|               0|        1|            14|\n",
      "|     NULL|          11|       11|      1|               4|        4|            11|\n",
      "|     NULL|          12|       12|      0|               2|        2|            11|\n",
      "|     NULL|          13|        9|      1|               0|        0|             9|\n",
      "|     NULL|          14|       12|      1|               0|        0|             9|\n",
      "|     NULL|          15|       11|      2|               1|        2|             7|\n",
      "|     NULL|          16|       11|      2|               3|        4|             6|\n",
      "|     NULL|          17|       12|      2|               3|        4|             4|\n",
      "|     NULL|          18|       11|      1|               3|        4|             7|\n",
      "|     NULL|          19|       13|      2|               2|        3|             7|\n",
      "|     NULL|          20|       11|      1|               0|        0|             5|\n",
      "|     NULL|          21|       11|      0|               1|        1|             3|\n",
      "|     NULL|          22|       16|      3|               0|        2|             4|\n",
      "|     NULL|          23|       21|      5|               3|        3|             4|\n",
      "|11-Sep-12|           0|        9|      3|               1|        1|             3|\n",
      "|     NULL|           1|        3|      2|               0|        1|             3|\n",
      "|     NULL|           2|        1|      1|               0|        0|             1|\n",
      "+---------+------------+---------+-------+----------------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91cc89b-332e-4903-bb8f-e6171c7cee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_Date: date (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Eyeballs: integer (nullable = true)\n",
      " |-- Zeroes: integer (nullable = true)\n",
      " |-- Completed_Trips: integer (nullable = true)\n",
      " |-- Requests: integer (nullable = true)\n",
      " |-- Unique_Drivers: integer (nullable = true)\n",
      " |-- Time_padded: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Cleaning the dataset\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lit, col, last, to_date, date_format, window, lpad, concat\n",
    "\n",
    "w = Window.orderBy(lit(1)).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_cleaned = df_trip.withColumn(\"dates\", last(col(\"Date\"), ignorenulls = True).over(w))\n",
    "\n",
    "df_final = df_cleaned.withColumn(\"Trip_Date\", date_format(to_date(col(\"dates\"), 'dd-MMM-yy'),'dd-MM-yyyy')) \\\n",
    "           .select('Trip_Date',col('Time (Local)').alias('Time'), col('Eyeballs ').alias('Eyeballs'),col('Zeroes ').alias('Zeroes'),col('Completed Trips ').alias('Completed_Trips'),col('Requests ').alias('Requests'),col('Unique Drivers').alias('Unique_Drivers'))\n",
    "df_final = df_final.withColumn('Time_padded', concat(col('Trip_Date'), lit(\" \"), lpad(col('Time').cast('string'),2,\"0\"), lit(\":00:00\")))\n",
    "df_final = df_final.withColumn('Trip_Date',to_date(col('Trip_Date'),'dd-MM-yyyy'))\n",
    "df_final.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35d78a5-6a03-4704-b3d7-9356723f69fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+------+---------------+--------+--------------+-------------------+\n",
      "| Trip_Date|Time|Eyeballs|Zeroes|Completed_Trips|Requests|Unique_Drivers|        Time_padded|\n",
      "+----------+----+--------+------+---------------+--------+--------------+-------------------+\n",
      "|2012-09-10|   7|       5|     0|              2|       2|             9|10-09-2012 07:00:00|\n",
      "|2012-09-10|   8|       6|     0|              2|       2|            14|10-09-2012 08:00:00|\n",
      "|2012-09-10|   9|       8|     3|              0|       0|            14|10-09-2012 09:00:00|\n",
      "|2012-09-10|  10|       9|     2|              0|       1|            14|10-09-2012 10:00:00|\n",
      "|2012-09-10|  11|      11|     1|              4|       4|            11|10-09-2012 11:00:00|\n",
      "|2012-09-10|  12|      12|     0|              2|       2|            11|10-09-2012 12:00:00|\n",
      "|2012-09-10|  13|       9|     1|              0|       0|             9|10-09-2012 13:00:00|\n",
      "|2012-09-10|  14|      12|     1|              0|       0|             9|10-09-2012 14:00:00|\n",
      "|2012-09-10|  15|      11|     2|              1|       2|             7|10-09-2012 15:00:00|\n",
      "|2012-09-10|  16|      11|     2|              3|       4|             6|10-09-2012 16:00:00|\n",
      "|2012-09-10|  17|      12|     2|              3|       4|             4|10-09-2012 17:00:00|\n",
      "|2012-09-10|  18|      11|     1|              3|       4|             7|10-09-2012 18:00:00|\n",
      "|2012-09-10|  19|      13|     2|              2|       3|             7|10-09-2012 19:00:00|\n",
      "|2012-09-10|  20|      11|     1|              0|       0|             5|10-09-2012 20:00:00|\n",
      "|2012-09-10|  21|      11|     0|              1|       1|             3|10-09-2012 21:00:00|\n",
      "|2012-09-10|  22|      16|     3|              0|       2|             4|10-09-2012 22:00:00|\n",
      "|2012-09-10|  23|      21|     5|              3|       3|             4|10-09-2012 23:00:00|\n",
      "|2012-09-11|   0|       9|     3|              1|       1|             3|11-09-2012 00:00:00|\n",
      "|2012-09-11|   1|       3|     2|              0|       1|             3|11-09-2012 01:00:00|\n",
      "|2012-09-11|   2|       1|     1|              0|       0|             1|11-09-2012 02:00:00|\n",
      "+----------+----+--------+------+---------------+--------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7c543bb-0451-4324-90d7-c33979045ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "| Trip_Date|Total_Completed_Trips|\n",
      "+----------+---------------------+\n",
      "|2012-09-22|                  248|\n",
      "+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Which date had the most completed trips during the two-week period?\n",
    "from pyspark.sql.functions import sum\n",
    "df_complete = df_final.groupBy('Trip_Date').agg(sum('Completed_Trips').alias(\"Total_Completed_Trips\"))\n",
    "df_complete.orderBy(\"Total_Completed_Trips\", ascending=False).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6566707a-2495-4b98-aa7d-e0c5255bf73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+------+---------------+--------+--------------+-------------------+\n",
      "| Trip_Date|Time|Eyeballs|Zeroes|Completed_Trips|Requests|Unique_Drivers|        Time_padded|\n",
      "+----------+----+--------+------+---------------+--------+--------------+-------------------+\n",
      "|2012-09-10|   7|       5|     0|              2|       2|             9|2012-09-10 07:00:00|\n",
      "|2012-09-10|   8|       6|     0|              2|       2|            14|2012-09-10 08:00:00|\n",
      "|2012-09-10|   9|       8|     3|              0|       0|            14|2012-09-10 09:00:00|\n",
      "|2012-09-10|  10|       9|     2|              0|       1|            14|2012-09-10 10:00:00|\n",
      "|2012-09-10|  11|      11|     1|              4|       4|            11|2012-09-10 11:00:00|\n",
      "|2012-09-10|  12|      12|     0|              2|       2|            11|2012-09-10 12:00:00|\n",
      "|2012-09-10|  13|       9|     1|              0|       0|             9|2012-09-10 13:00:00|\n",
      "|2012-09-10|  14|      12|     1|              0|       0|             9|2012-09-10 14:00:00|\n",
      "|2012-09-10|  15|      11|     2|              1|       2|             7|2012-09-10 15:00:00|\n",
      "|2012-09-10|  16|      11|     2|              3|       4|             6|2012-09-10 16:00:00|\n",
      "|2012-09-10|  17|      12|     2|              3|       4|             4|2012-09-10 17:00:00|\n",
      "|2012-09-10|  18|      11|     1|              3|       4|             7|2012-09-10 18:00:00|\n",
      "|2012-09-10|  19|      13|     2|              2|       3|             7|2012-09-10 19:00:00|\n",
      "|2012-09-10|  20|      11|     1|              0|       0|             5|2012-09-10 20:00:00|\n",
      "|2012-09-10|  21|      11|     0|              1|       1|             3|2012-09-10 21:00:00|\n",
      "|2012-09-10|  22|      16|     3|              0|       2|             4|2012-09-10 22:00:00|\n",
      "|2012-09-10|  23|      21|     5|              3|       3|             4|2012-09-10 23:00:00|\n",
      "|2012-09-11|   0|       9|     3|              1|       1|             3|2012-09-11 00:00:00|\n",
      "|2012-09-11|   1|       3|     2|              0|       1|             3|2012-09-11 01:00:00|\n",
      "|2012-09-11|   2|       1|     1|              0|       0|             1|2012-09-11 02:00:00|\n",
      "+----------+----+--------+------+---------------+--------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+---------------------+\n",
      "|              window|Total_Completed_Trips|\n",
      "+--------------------+---------------------+\n",
      "|{2012-09-22 05:30...|                  257|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##What was the highest number of completed trips within a 24-hour period?\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "df_final = df_final.withColumn('Time_padded',to_timestamp(col('Time_padded'),'dd-MM-yyyy HH:mm:ss'))\n",
    "\n",
    "df_final.show()\n",
    "df_window_24hr = df_final.groupBy(window('Time_padded', '24 hours')).agg(sum('Completed_Trips').alias(\"Total_Completed_Trips\"))\n",
    "df_window_24hr.orderBy(\"Total_Completed_Trips\",ascending=False).limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62839ac6-2e6f-4291-b134-37bfd173e0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|Hour|Total_Requests|\n",
      "+----+--------------+\n",
      "|  23|           184|\n",
      "+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Which hour of the day had the most requests during the two-week period?\n",
    "\n",
    "df_most_request = df_final.groupBy(col('Time').alias('Hour')).agg(sum('Requests').alias('Total_Requests')).orderBy('Total_Requests', ascending=False)\n",
    "df_most_request.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d69013-ec6f-4ee9-b930-33369c84325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total zeroes: 1429\n",
      "Total zeroes during weekend: 644\n",
      "Percentage: 45.07\n"
     ]
    }
   ],
   "source": [
    "##What percentages of all zeroes during the two-week period occurred on weekends (Friday at 5 pm to Sunday at 3 am)?\n",
    "from pyspark.sql.functions import dayofweek\n",
    "df_final = df_final.withColumn('Day', dayofweek('Trip_Date'))\n",
    "#df_final.printSchema()\n",
    "#df_final.show()\n",
    "\n",
    "total_zeroes = df_final.agg(sum('Zeroes')).collect()[0][0]\n",
    "weekend_data = df_final.filter((col('Day') == 7) | ((col('Day') == 6) & (col('Time') >= 17)) | ((col('Day') == 1) & (col('Time') <= 3)))\n",
    "weekend_zeroes = weekend_data.agg(sum('Zeroes')).collect()[0][0]\n",
    "\n",
    "#weekend_data.show()\n",
    "\n",
    "print(\"Total zeroes:\", total_zeroes)\n",
    "print(\"Total zeroes during weekend:\",weekend_zeroes)\n",
    "print(\"Percentage:\", round((weekend_zeroes/total_zeroes)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ffc418-a2b8-44ba-85ce-6d7736ca521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Completed Trips: 1365\n",
      "Total Weighted Ratio: 779.1\n",
      "Average Weighted Ratio: 0.57\n"
     ]
    }
   ],
   "source": [
    "#What is the weighted average ratio of completed trips per driver during the two-week period?\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "ratio_data = df_final.withColumn('completed_to_driver', (col('Completed_Trips')/col('Unique_Drivers')))\n",
    "ratio_data = ratio_data.groupBy('Time').agg(avg('completed_to_driver').alias('avg_completed_to_driver'), sum('Completed_Trips').alias('total_completed_trips'))\n",
    "ratio_data = ratio_data.withColumn('weighted_avg', col('avg_completed_to_driver')*col('total_completed_trips'))\n",
    "\n",
    "total_completed_trips = ratio_data.agg(sum('total_completed_trips')).collect()[0][0]\n",
    "total_weighted_ratio = ratio_data.agg(sum('weighted_avg')).collect()[0][0]\n",
    "#ratio_data.show()\n",
    "print('Total Completed Trips:',total_completed_trips)\n",
    "print('Total Weighted Ratio:', round(total_weighted_ratio,3))\n",
    "print('Average Weighted Ratio:',round(total_weighted_ratio/total_completed_trips,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acd213de-a97a-4dac-bb4d-aced65b7a584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-------------+\n",
      "|Time|unique_request|total_request|\n",
      "+----+--------------+-------------+\n",
      "|   0|            11|           51|\n",
      "|   1|             8|           46|\n",
      "|   2|             8|           44|\n",
      "|   3|             7|           40|\n",
      "|   4|             3|           41|\n",
      "|   5|             4|           46|\n",
      "|   6|             5|           50|\n",
      "|   7|             5|           53|\n",
      "|   8|             6|           56|\n",
      "|   9|             6|           58|\n",
      "|  10|             4|           62|\n",
      "|  11|             8|           67|\n",
      "|  12|             8|           69|\n",
      "|  13|             8|           73|\n",
      "|  14|             8|           75|\n",
      "|  15|             8|           77|\n",
      "|  16|             8|           77|\n",
      "|  17|            10|           69|\n",
      "|  18|             9|           59|\n",
      "|  19|            10|           50|\n",
      "+----+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period \n",
    "#in terms of unique requests? A new shift starts every 8 hours. Assume that a driver will work the same shift each day.\n",
    "from pyspark.sql.functions import countDistinct\n",
    "request_data = df_final.groupBy('Time').agg(countDistinct('Requests').alias('unique_request'))\n",
    "\n",
    "eight_hr_window = Window.orderBy(col('Time')).rowsBetween(0,7)\n",
    "request_data = request_data.withColumn('total_request', sum(col('unique_Request')).over(eight_hr_window))\n",
    "request_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d580e34-e55f-42bf-b553-a1a3526734ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------------+------------------+\n",
      "|              window|total_eyeballs|total_zeroes|zeroes_to_eyeballs|\n",
      "+--------------------+--------------+------------+------------------+\n",
      "|{2012-09-08 05:30...|           205|          35|              0.17|\n",
      "|{2012-09-11 05:30...|          1080|         211|               0.2|\n",
      "|{2012-09-14 05:30...|          1756|         439|              0.25|\n",
      "|{2012-09-17 05:30...|          1029|         210|               0.2|\n",
      "|{2012-09-20 05:30...|          2220|         440|               0.2|\n",
      "|{2012-09-23 05:30...|           397|          94|              0.24|\n",
      "+--------------------+--------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?\n",
    "from pyspark.sql.functions import round\n",
    "df_zero_to_eyeball = df_final.groupBy(window('Time_padded', '72 hours')).agg(sum('Eyeballs').alias('total_eyeballs'), sum('Zeroes').alias('total_zeroes'))\n",
    "df_zero_to_eyeball = df_zero_to_eyeball.withColumn('zeroes_to_eyeballs', round((col('total_zeroes')/col('total_eyeballs')),2))\n",
    "df_zero_to_eyeball.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ed8757d-86f4-4477-9a23-c3cff2db61c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+-----------------+\n",
      "|Time|       Avg_Request|        Avg_Driver|request_to_driver|\n",
      "+----+------------------+------------------+-----------------+\n",
      "|   2| 7.142857142857143| 4.428571428571429|             1.61|\n",
      "|  23|13.142857142857142|               8.5|             1.55|\n",
      "|   0|10.142857142857142| 7.928571428571429|             1.28|\n",
      "|   5|               1.0|0.7857142857142857|             1.27|\n",
      "|  22|12.428571428571429|10.285714285714286|             1.21|\n",
      "|   1| 6.857142857142857| 6.714285714285714|             1.02|\n",
      "|   4|0.6428571428571429|0.6428571428571429|              1.0|\n",
      "|   3|               2.5| 2.857142857142857|             0.88|\n",
      "|  19|11.142857142857142|12.857142857142858|             0.87|\n",
      "|   6|               2.0| 2.642857142857143|             0.76|\n",
      "|  21|               8.0|11.071428571428571|             0.72|\n",
      "|  18|               8.5|12.428571428571429|             0.68|\n",
      "|  20| 7.642857142857143|11.642857142857142|             0.66|\n",
      "|  17|               7.0|11.785714285714286|             0.59|\n",
      "|  16| 5.857142857142857|10.285714285714286|             0.57|\n",
      "|  14| 5.071428571428571| 8.928571428571429|             0.57|\n",
      "|  15| 5.071428571428571| 9.928571428571429|             0.51|\n",
      "|  13|3.9285714285714284| 8.714285714285714|             0.45|\n",
      "|  12|3.7857142857142856| 9.428571428571429|              0.4|\n",
      "|   7|1.5714285714285714| 4.285714285714286|             0.37|\n",
      "+----+------------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If you could add 5 drivers to any single hour of every day during the two-week period, \n",
    "#which hour should you add them to? Hint: Consider both rider eyeballs and driver supply when choosing.\n",
    "\n",
    "df_req_to_driver = df_final.groupBy('Time').agg(avg('Requests').alias('Avg_Request'), avg('Unique_Drivers').alias('Avg_Driver'))\n",
    "df_req_to_driver = df_req_to_driver.withColumn('request_to_driver', round(col('Avg_Request')/col('Avg_Driver'), 2))\n",
    "\n",
    "df_req_to_driver.orderBy('request_to_driver', ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3ebdf4-9ad9-4e1e-b8dd-255402419d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+\n",
      "|time|Avg_Completed_Trips|Avg_Unique_Drivers|\n",
      "+----+-------------------+------------------+\n",
      "|   4|0.14285714285714285|0.6428571428571429|\n",
      "|   5| 0.2857142857142857|0.7857142857142857|\n",
      "|  10| 1.2857142857142858| 9.214285714285714|\n",
      "|   6| 1.3571428571428572| 2.642857142857143|\n",
      "|   7| 1.3571428571428572| 4.285714285714286|\n",
      "|   9| 1.4285714285714286| 7.857142857142857|\n",
      "|   3|                1.5| 2.857142857142857|\n",
      "|   8| 1.7142857142857142| 6.785714285714286|\n",
      "|  11| 2.5714285714285716|               9.5|\n",
      "|  13|  3.142857142857143| 8.714285714285714|\n",
      "|  12| 3.2857142857142856| 9.428571428571429|\n",
      "|  15|  3.357142857142857| 9.928571428571429|\n",
      "|  14|                3.5| 8.928571428571429|\n",
      "|   2|  4.357142857142857| 4.428571428571429|\n",
      "|  16|  4.857142857142857|10.285714285714286|\n",
      "|   1|  5.071428571428571| 6.714285714285714|\n",
      "|  20|  5.428571428571429|11.642857142857142|\n",
      "|  17|  5.571428571428571|11.785714285714286|\n",
      "|  21|  6.285714285714286|11.071428571428571|\n",
      "|  18|  6.571428571428571|12.428571428571429|\n",
      "+----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the data from all two weeks, which time might make the most sense to consider a true “end day” instead of midnight? \n",
    "#(i.e when are supply and demand at both their natural minimums)\n",
    "\n",
    "df_trip_driver = df_final.groupBy('time').agg(avg('Completed_Trips').alias('Avg_Completed_Trips'), avg('Unique_Drivers').alias('Avg_Unique_Drivers'))\n",
    "\n",
    "df_trip_driver.orderBy('Avg_Completed_Trips','Avg_Unique_Drivers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77022131-c061-4ab4-9d26-87117be418f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_Date: date (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Eyeballs: integer (nullable = true)\n",
      " |-- Zeroes: integer (nullable = true)\n",
      " |-- Completed_Trips: integer (nullable = true)\n",
      " |-- Requests: integer (nullable = true)\n",
      " |-- Unique_Drivers: integer (nullable = true)\n",
      " |-- Time_padded: timestamp (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52909544-3e06-4d7d-aabe-b0c623cecc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
